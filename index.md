---
layout: default
---

Hello!
I am a Ph.D student in CSE at Seoul National University, and studying under the supervision of **[Prof. Gunhee Kim](https://vision.snu.ac.kr/gunhee/)**.

My research focuses on **Visual Localization** and **3D Dense Captioning** for enhanced 3D scene understanding, with particular interest in: 
(i) understanding complex scenes from images and point clouds, 
(ii) effectively handling multi-modalities, 
and (iii) achieving a comprehensive understanding of 3D scenes through natural language.



### Education

<h4 class="education">
  <i class="material-icons md-18">account_balance</i>
  <a href="http://en.snu.ac.kr/">Seoul National University (SNU)</a>, Seoul, Korea
  <sup>2018.03 - 2025.02</sup>
</h4>

- Integrated M.S./Ph.D. student in [Computer Science and Engineering]
- Cumulative GPA: 4.03 / 4.3 (4.23 / 4.5)
- Advisor: **[Prof. Gunhee Kim](https://vision.snu.ac.kr/gunhee/)**
- **Outstanding Doctoral Thesis Award ðŸŽ“âœ¨**

[Computer Science and Engineering]: https://cse.snu.ac.kr/en

<h4 class="education">
  <i class="material-icons md-18">school</i>
  <a href="https://wwwe.sogang.ac.kr/wwwe/index_new.html">Sogang University</a>, Seoul, Korea
  <sup>2014.03 - 2018.02</sup>
</h4>

- B.S. in [Computer Science and Engineering]
- Cumulative GPA: 3.58 / 4.3 (3.88 / 4.5), **Magna Cum Laude**
- Advisor: **[Prof. Hyukjun Lee](http://ecl.sogang.ac.kr)**

[Computer Science and Engineering]: https://ecs.sogang.ac.kr/ecs/index_new.html



### Publications

- **Bi-directional Contextual Attention for 3D Dense Captioning** <br/>
**Minjung Kim**, Hyung Suk Lim, Soonyoung Lee, Bumsoo Kim<sup>\*</sup>, Gunhee Kim<sup>\*</sup> <br/>
ECCV 2024 (**Oral presentation**)
<a class="code" href="https://arxiv.org/abs/2408.06662">[arXiv]</a>
<a class="code" href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/02804.pdf">[pdf]</a>
<a class="code" href="https://eccv.ecva.net/virtual/2024/poster/114">[poster]</a>
<a class="code" href="https://minnjung.github.io/BiCA">[HTML]</a>

- **Rethinking the Role of Queries in 3D Dense Captioning** <br/>
**Minjung Kim**, Gunhee Kim <br/>
KCC 2024
<a class="code" href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE11862160">[pdf]</a>

- **See It All: Contextualized Late Aggregation for 3D Dense Captioning** <br/>
**Minjung Kim**, Hyung Suk Lim, Seung Hwan Kim, Soonyoung Lee, <br/>
Bumsoo Kim<sup>\*</sup>, Gunhee Kim<sup>\*</sup> <br/>
ACL 2024 Findings
<a class="code" href="https://openreview.net/forum?id=NVhRn_B29i">[OpenReview]</a>
<a class="code" href="https://arxiv.org/abs/2408.07648">[arXiv]</a>
<a class="code" href="https://aclanthology.org/2024.findings-acl.202/">[pdf]</a>
<a class="code" href="https://underline.io/events/466/posters/18354/poster/102446-see-it-all-contextualized-late-aggregation-for-3d-dense-captioning?tab=Video">[Underline]</a>

- **EP2P-Loc: End-to-End 3D Point to 2D Pixel Localization for Large-Scale Visual Localization** <br/>
**Minjung Kim**, Junseo Koo, Gunhee Kim <br/>
ICCV 2023
<a class="code" href="http://arxiv.org/abs/2309.07471">[arXiv]</a>
<a class="code" href="https://openaccess.thecvf.com/content/ICCV2023/papers/Kim_EP2P-Loc_End-to-End_3D_Point_to_2D_Pixel_Localization_for_Large-Scale_ICCV_2023_paper.pdf">[pdf]</a>
<a class="code" href="https://github.com/minnjung/EP2P-Loc">[code]</a>

- **Indoor/Outdoor Transition Recognition Based on Door Detection** <br/>
Seohyun Jeon, **Minjung Kim**, Seunghwan Park, Jaeyoung Lee <br/>
UR 2022
<a class="code" href="https://ieeexplore.ieee.org/abstract/document/9826236">[pdf]</a>

- **Drop-Bottleneck: Learning Discrete Compressed Representation for Noise-Robust Exploration** <br/>
Jaekyeom Kim, **Minjung Kim**, Dongyeon Woo, Gunhee Kim <br/>
ICLR 2021
<a class="code" href="https://openreview.net/forum?id=1rxHOBjeDUW">[OpenReview]</a>
<a class="code" href="https://arxiv.org/abs/2103.12300">[arXiv]</a>
<a class="code" href="https://openreview.net/pdf?id=1rxHOBjeDUW">[pdf]</a>
<a class="code" href="https://iclr.cc/virtual/2021/poster/3127">[talk]</a>
<a class="code" href="https://github.com/jaekyeom/drop-bottleneck">[code]</a>

- **Logo Detection and Recognition Algorithm using YOLO-v3 Model** <br/>
**Minjung Kim**, Sungen Kim, Gunhee Kim <br/>
CICS 2020
<a class="code" href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE10492565">[pdf]</a>

- **Memorization Precedes Generation: Learning Unsupervised GANs with Memory Networks** <br/>
Youngjin Kim, **Minjung Kim**, Gunhee Kim <br/>
ICLR 2018
<a class="code" href="https://openreview.net/forum?id=rkO3uTkAZ">[OpenReview]</a>
<a class="code" href="https://arxiv.org/abs/1803.01500">[arXiv]</a>
<a class="code" href="https://openreview.net/pdf?id=rkO3uTkAZ">[pdf]</a>
<a class="code" href="https://github.com/whyjay/memoryGAN">[code]</a>

- **Machine Learning for Determining Duplicate Question** <br/>
**Minjung Kim**, Yeongjoon Park, Hyung Suk Lim, Jihoon Yang <br/>
KCS 2017
<a class="code" href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE07322773">[pdf]</a>

- **Sketch based Face Image Generation with Text Mapping** <br/>
**Minjung Kim**, Hyung Suk Lim, Yeongjoon Park, Yeseul Joo, Myoung Wan Koo <br/>
KCS 2017
<a class="code" href="https://www.dbpia.co.kr/journal/articleDetail?nodeId=NODE07322778">[pdf]</a>


### Experiences

<h4 class="experiences">
  <i class="material-icons md-18">work</i>
  <a href="https://www.lgresearch.ai/ourwork/research?tab=PG">LG AI Research</a>, Seoul, Korea
  <sup> 2023.06 - 2024.05 </sup>
</h4>

- Vision and Multimodal Lab
- Research intern



### Awards & Scholarships

- **Outstanding Doctoral Thesis Award** at Dept. of Computer Science and Engineering, Seoul National University, 2025
- [**YoulChon AI Star Fellowship**](https://aiis.snu.ac.kr/bbs/board.php?bo_table=sub5_1&wr_id=555) at YoulChon Foundation, Nongshim Group, 2024
- [**Animal Datathon Korea**](https://blog.naver.com/aidkr/222518300737) (2nd place, 2021)
- [**Samsung Humantech Paper Award**](https://humantech.samsung.com/saitext/intro.do) (Silver Prize in Signal Processing, 2021)
- [**KSC 2017 Paper Award**](http://www.kiise.or.kr/conference/board/boardview.do?CC=KSC&CS=2017&PARENT_ID=011400&NUM=104) (Encouragement Award, 2018)
- **Magna Cum Laude Honor** at Sogang University, 2018
- **Academic Excellence Scholarship** at Sogang University, 2017 and 2018.
- **Windows 10 IoT Core & Microsoft Azure for Microsoft IoT Solution Competition** (10th place, 2017)


<!--
### Development Projects

- **DeepGuider** <sup>2019.02 - 2023.04</sup> <a class="code" href="https://github.com/deepguider/DeepGuider">[code]</a> <br/>
The DeepGuider Project is a national government-funded research project focused on developing a navigation guidance system for robots to navigate urban environments without pre-mapping.
I contribute to finding clues to locate autonomous robots by detecting and recognizing points of interests (POIs) in images of a scene, including text, landmarks, and doors for indoor-outdoor transition, while also developing robust training methods for environmental changes.

- **PRIDE: 3D Place Recognition In Dynamic Environment** <sup>2022.03 - Current</sup> <a class="code" href="https://github.com/minnjung/PRIDE">[code]</a> <br/>
This work proposes a new dataset called PRIDE, which includes dynamic objects such as cars and pedestrians, for 3D place recognition in dynamic environments that are more realistic and challenging than current benchmark datasets.
The proposed PRIDE-Net architecture with a new loss function focuses on extracting discriminative global descriptors and capturing global context using spatial information, while being robust to dynamic environments.
Experiments on the PRIDE dataset and existing benchmarks show that our proposed method outperforms previous methods and that each proposed module effectively improves performance.
The code will be released after acceptance.

- **FCAT: Fully Convolutional Network with Self-Attention for Point Cloud based Place Recognition** <sup>2020.12 - 2022.02</sup> <a class="code" href="https://github.com/minnjung/FCAT">[code]</a> <br/>
We construct a novel network named FCAT (Fully Convolutional network with a self-ATtention unit) that can generate a discriminative and context-aware global descriptor for place recognition from the 3D point cloud.
It features with a novel sparse fully convolutional network architecture with sparse tensors for extracting informative local geometric features computed in a single pass.
It also involves a self-attention module for 3D point cloud to encode local context information between local descriptors.
The code will be released after acceptance.

- **Bayesian Deep Learning course** <sup>2018.02 - 2018.07</sup> <a class="code" href="https://www.edwith.org/bayesiandeeplearning">[lecture]</a> <br/>
To understand deep learning papers, we explain the basic concepts of probability and Bayesian, and introduce papers related to Bayesian neural networks.
This lecture can be taken through [edwith](https://www.edwith.org/) of Naver Connect.

- **Sketch based Face Image Generation with Text Mapping** <sup>2017.09 - 2018.02</sup> <a class="code" href="https://github.com/hyungsuklim/metamon">[code]</a> <br/>
A typical sketch might have been uncomfortable when a person or program was used to map a person's features in detail.
This process is limited not only because it is very complex and requires technicians, but also because it creates a feeling of incompatibility with real people.
This program, named Metamon, makes a picture of a person's face by entering the image of the border sketch of the person's face and the text information that shows the characteristics of the face.

- **Arduino & Raspberry Pi & Internet of Things (IoT) Tutorial** <sup>2016.12 - 2017.03</sup> <a class="code" href="http://makewith.co/page/user/1152/profile">[project]</a> <br/>
I create tutorial pages with Youtube videos and code for beginners in Arduino kit and Raspberry Pi development.
I introduce the concept of the Internet of Things (IoT) and work on a mini-project using [ThingSpeak](https://thingspeak.com/).

- **Sogang Navigation and Introduction (SNI)** <sup>2015.03 - 2015.07</sup> <a class="code" href="https://github.com/hyungsuklim/SGCS-Assignment/tree/master/2015/Data-Structure_2015/Project">[code]</a> <br/>
We develop a navigation system that introduces the internal facilities of each building and displays the shortest route and time from building to building using the Floyd-Washall algorithm.
To build data for the development, we measured the time taken by walking directly on each path.

-->


### Professional Activities

- **Reviewer of International Conferences** <br/>
European Conference on Computer Vision (ECCV) 2024 <br/>
International Conference on Computer Vision (ICCV) 2023 <br/>
Conference on Computer Vision and Pattern Recognition (CVPR) 2023, 2025 <br/>
Asian Conference on Computer Vision (ACCV) 2022 <br/>
International Conference on Learning Representations (ICLR) 2022, 2023 <br/>
Neural Information Processing Systems (NeurIPS) 2021, 2022, 2023, 2024 <br/>

- **Reviewer of International Journals** <br/>
International Journal of Computer Vision (IJCV) 2024 <br/>

- **Technical Coaching** <br/>
2022-3 SK hynix ML Engineer Technical Coaching

